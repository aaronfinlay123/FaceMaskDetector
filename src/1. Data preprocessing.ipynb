{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training any sort of model using a machine learning algorithm, a large dataset is first needed to train that model off of. Data can be anything that would help benefit with the training of the model. In this case, images of people facing the camera head on wearing/not wearing a face mask is the type of data that is being used.\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "Raw data is first collected. These are just images of people wearing/not wearing face masks. This is not enough by itself as the data must the be divided into two groups, i.e into 'with_mask' and 'without_mask'.\n",
    "\n",
    "# Categorisation and labeling\n",
    "\n",
    "Next the data must be categorised and labeled as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'without_mask': 0, 'with_mask': 1}\n",
      "['without_mask', 'with_mask']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "\n",
    "data_path='dataset'\n",
    "categories=os.listdir(data_path)\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing and reshaping the data\n",
    "When feeding any sort of data into an algorithm, it is important that we normalise the data. In this case of working with many images in the dataset, the images must be resized and shaped so that they are all fixed and common in size. For the purpose of this project, each image was resized to be 50 pixels by 50 pixels and were converted to greyscale. It is also important to note that each image was added to the data array, and its corresponding label is added to the target array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "\n",
    "        try:\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n",
    "            resized=cv2.resize(gray,(img_size,img_size))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialising the resulting pre processed data\n",
    "Now that the dataset has been preprocessed and sorted into arrays, it must now be serialised so it can be used in the training process. To serialise the data, numpy is used as it is capable of serialising arrays and deserialising them later on for use (also known as flattening and unflattening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target=np.array(target)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "new_target=np_utils.to_categorical(target)\n",
    "\n",
    "np.save('data',data)\n",
    "np.save('target',new_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing up\n",
    "Now that the data has been preprocessed and serialised, it is now ready to be used in the training process. It is important to note that this must be done any time data needs to be added or removed from the dataset so it is not uncommon to have to use this multiple times throughout the development of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
